# Environment configuration for LLM API
# Copy this file to .env and customize for your setup

# LLM Provider Selection (ollama | azure)
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_MODEL=phi4-mini
OLLAMA_BASE_URL=http://localhost:11434

# Azure OpenAI Configuration
# AZURE_OPENAI_MODEL=gpt-4.1-mini
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_KEY=your-api-key
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Application Configuration
LOG_LEVEL=INFO
APP_HOST=0.0.0.0
APP_PORT=8000

# Enable PII Masking
PII_MASKING_ENABLED=true

# OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT="http://otelcol:4318"
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
OTEL_SERVICE_NAME=fastapi-chatbot
OTEL_LOG_LEVEL=WARNING # Reduce verbose logging for known async context issues
OTEL_METRIC_EXPORT_INTERVAL=1000
