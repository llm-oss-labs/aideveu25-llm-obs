services:
  llm-workshop-api:
    build:
      context: .
      dockerfile: apps/api/Dockerfile
    ports:
      - "8000:8000"
    env_file: .env
    restart: unless-stopped

  llm-workshop-cli:
    build:
      context: .
      dockerfile: apps/cli/Dockerfile
    environment:
      - API_BASE_URL=http://llm-workshop-api:8000/v1
    depends_on:
      - llm-workshop-api
    stdin_open: true # keep STDIN open for interactive CLI
    tty: true # allocate a pseudo-TTY for nice prompts

  otelcol:
    build: ./apps/otel_col
    container_name: otelcol
    deploy:
      resources:
        limits:
          memory: 125M
    restart: unless-stopped
    volumes:
      - ./apps/otel_col/otel_config.yaml:/etc/otelcol-config.yml
    ports:
      - "4317:4317" # OTLP gRPC receiver
      - "4318:4318" # OTLP http receiver
      # - "9464:9464"   # Optional: expose Prometheus scrape endpoint to host
    depends_on:
      - tempo

  grafana:
    image: grafana/grafana:12.1.1
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000" # Grafana UI
    volumes:
      - ./apps/grafana/provisioning:/etc/grafana/provisioning
      - ./apps/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
      - ./apps/grafana/dashboards:/etc/grafana/dashboards:ro
      # - grafana-data:/var/lib/grafana
    depends_on:
      - tempo
      - prometheus

  tempo:
    image: grafana/tempo:main-a909571
    container_name: tempo
    restart: unless-stopped
    ports:
      - "3200:3200" # Tempo HTTP API
      - "9095:9095" # Tempo gRPC API
    volumes:
      - ./apps/grafana_tempo/tempo.yaml:/etc/tempo.yaml:ro
      #- tempo-data:/tmp/tempo
    command: [ "-config.file=/etc/tempo.yaml" ]

  prometheus:
    image: prom/prometheus:v3.5.0
    container_name: prometheus
    restart: unless-stopped
    depends_on:
      - otelcol
    ports:
      - "9090:9090" # Prometheus UI
    volumes:
      - ./apps/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro

volumes:
  tempo-data:
  grafana-data:
