{
    "annotations": {
        "list": [
            {
                "builtIn": 1,
                "datasource": {
                    "type": "grafana",
                    "uid": "-- Grafana --"
                },
                "enable": true,
                "hide": true,
                "iconColor": "rgba(0, 211, 255, 1)",
                "name": "Annotations & Alerts",
                "type": "dashboard"
            }
        ]
    },
    "editable": true,
    "fiscalYearStartMonth": 0,
    "graphTooltip": 1,
    "id": 0,
    "links": [
        {
            "asDropdown": false,
            "icon": "bolt",
            "includeVars": false,
            "keepTime": false,
            "tags": [],
            "targetBlank": true,
            "title": "OpenLIT Github",
            "tooltip": "Github",
            "type": "link",
            "url": "https://github.com/openlit/openlit"
        },
        {
            "asDropdown": false,
            "icon": "doc",
            "includeVars": false,
            "keepTime": false,
            "tags": [
                "AI"
            ],
            "targetBlank": true,
            "title": "OpenLIT Docs",
            "tooltip": "Documentation",
            "type": "link",
            "url": "https://docs.openlit.io/"
        }
    ],
    "panels": [
        {
            "fieldConfig": {
                "defaults": {},
                "overrides": []
            },
            "gridPos": {
                "h": 4,
                "w": 12,
                "x": 0,
                "y": 0
            },
            "id": 21,
            "options": {
                "code": {
                    "language": "plaintext",
                    "showLineNumbers": false,
                    "showMiniMap": false
                },
                "content": "---\n# GenAI Observability\n\nThis dashboard displays the usage stats of LLMs, tracking OpenTelemetry Traces and Metrics sent using [OpenLIT](https://github.com/openlit/openlit).\n\n---",
                "mode": "markdown"
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "title": "",
            "transparent": true,
            "type": "text"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the LLM Request Rate",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "fixedColor": "blue",
                        "mode": "palette-classic-by-name"
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "red",
                                "value": 0
                            },
                            {
                                "color": "#EAB839",
                                "value": 10
                            },
                            {
                                "color": "#6ED0E0",
                                "value": 100
                            }
                        ]
                    },
                    "unit": "reqps"
                },
                "overrides": []
            },
            "gridPos": {
                "h": 4,
                "w": 12,
                "x": 12,
                "y": 0
            },
            "id": 22,
            "options": {
                "colorMode": "background",
                "graphMode": "area",
                "justifyMode": "auto",
                "orientation": "auto",
                "percentChangeColorMode": "standard",
                "reduceOptions": {
                    "calcs": [
                        "lastNotNull"
                    ],
                    "fields": "",
                    "values": false
                },
                "showPercentChange": true,
                "textMode": "value_and_name",
                "wideLayout": true
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "disableTextWrap": false,
                    "editorMode": "code",
                    "expr": "sum(rate(gen_ai_requests_total{telemetry_sdk_name=\"openlit\"}[$__rate_interval]))",
                    "fullMetaSearch": false,
                    "includeNullMetadata": true,
                    "instant": false,
                    "legendFormat": "LLM Request Rate",
                    "range": true,
                    "refId": "A",
                    "useBackend": false
                }
            ],
            "title": "",
            "transparent": true,
            "type": "stat"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the cost incurred from using GenAI models. Note: OpenLIT applies estimated costs to all requests (including free local Ollama models) for tracking purposes. When using actual paid APIs like Azure OpenAI, these costs will reflect real charges.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "fixedColor": "blue",
                        "mode": "shades"
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "red",
                                "value": 80
                            }
                        ]
                    },
                    "unit": "currencyUSD",
                    "decimals": 6
                },
                "overrides": []
            },
            "gridPos": {
                "h": 4,
                "w": 8,
                "x": 0,
                "y": 4
            },
            "id": 2,
            "options": {
                "colorMode": "background",
                "graphMode": "area",
                "justifyMode": "auto",
                "orientation": "auto",
                "percentChangeColorMode": "standard",
                "reduceOptions": {
                    "calcs": [
                        "lastNotNull"
                    ],
                    "fields": "",
                    "values": false
                },
                "showPercentChange": true,
                "textMode": "auto",
                "wideLayout": true
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "sum(gen_ai_usage_cost_USD_sum{telemetry_sdk_name=\"openlit\", deployment_environment=~\"$environment\", service_name=~\"$application\"}) / 100000 OR on() vector(0)",
                    "hide": false,
                    "legendFormat": "Total Usage Cost",
                    "range": true,
                    "refId": "A"
                }
            ],
            "title": "",
            "transparent": true,
            "type": "stat"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the total number of successful requests made to the GenAI system. A successful request is one that is completed without errors, indicating seamless operation and effective utilization of the GenAI service. Tracking this helps in understanding the reliability and performance of the GenAI system.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "mode": "palette-classic-by-name"
                    },
                    "mappings": [],
                    "min": 0,
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "red",
                                "value": 80
                            }
                        ]
                    },
                    "unit": "none"
                },
                "overrides": []
            },
            "gridPos": {
                "h": 4,
                "w": 8,
                "x": 8,
                "y": 4
            },
            "id": 1,
            "options": {
                "colorMode": "background",
                "graphMode": "area",
                "justifyMode": "auto",
                "orientation": "auto",
                "percentChangeColorMode": "standard",
                "reduceOptions": {
                    "calcs": [
                        "lastNotNull"
                    ],
                    "fields": "",
                    "values": false
                },
                "showPercentChange": true,
                "textMode": "auto",
                "wideLayout": true
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "sum by() (gen_ai_requests_total{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"})",
                    "hide": false,
                    "legendFormat": "Total Successful GenAI Requests",
                    "range": true,
                    "refId": "A"
                }
            ],
            "title": "",
            "transparent": true,
            "type": "stat"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the total number of tokens consumed by GenAI requests, providing a direct measure of usage. Monitoring this helps in assessing the demand on GenAI services and guiding resource allocation or optimization strategies.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "fixedColor": "purple",
                        "mode": "shades"
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "red",
                                "value": 80
                            }
                        ]
                    },
                    "unit": "none"
                },
                "overrides": []
            },
            "gridPos": {
                "h": 4,
                "w": 8,
                "x": 16,
                "y": 4
            },
            "id": 3,
            "options": {
                "colorMode": "background",
                "graphMode": "area",
                "justifyMode": "auto",
                "orientation": "auto",
                "percentChangeColorMode": "standard",
                "reduceOptions": {
                    "calcs": [
                        "lastNotNull"
                    ],
                    "fields": "",
                    "values": false
                },
                "showPercentChange": true,
                "textMode": "auto",
                "wideLayout": true
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "editorMode": "code",
                    "expr": "sum(gen_ai_client_token_usage_sum{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"})",
                    "hide": false,
                    "legendFormat": "Total Usage Tokens",
                    "range": true,
                    "refId": "A"
                }
            ],
            "title": "",
            "transparent": true,
            "type": "stat"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the average cost per use of the GenAI models and related services. It provides insights into the cost-effectiveness of interactions with GenAI, helping to identify trends in expense per operation. Monitoring this assists in optimizing budget allocation and improving cost efficiency in GenAI utilization.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "fixedColor": "blue",
                        "mode": "shades"
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "#EAB839",
                                "value": 0.5
                            },
                            {
                                "color": "red",
                                "value": 1
                            }
                        ]
                    },
                    "unit": "currencyUSD",
                    "decimals": 6
                },
                "overrides": []
            },
            "gridPos": {
                "h": 2,
                "w": 8,
                "x": 0,
                "y": 8
            },
            "id": 5,
            "options": {
                "colorMode": "background",
                "graphMode": "none",
                "justifyMode": "auto",
                "orientation": "auto",
                "percentChangeColorMode": "standard",
                "reduceOptions": {
                    "calcs": [
                        "lastNotNull"
                    ],
                    "fields": "",
                    "values": false
                },
                "showPercentChange": true,
                "textMode": "auto",
                "wideLayout": true
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "avg by() (gen_ai_usage_cost_USD_sum{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"}) / 100000 OR on() vector(0)",
                    "hide": false,
                    "legendFormat": "Avg Usage Cost",
                    "range": true,
                    "refId": "A"
                }
            ],
            "title": "",
            "transparent": true,
            "type": "stat"
        },
        {
            "datasource": {
                "type": "tempo",
                "uid": "${tempo_datasource}"
            },
            "description": "This panel displays the distribution of request durations for GenAI services. It highlights how long requests take to complete, from the shortest to the longest durations, offering insights into system performance and efficiency. Understanding this distribution helps in identifying bottlenecks and optimizing response times.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "fixedColor": "blue",
                        "mode": "palette-classic"
                    },
                    "custom": {
                        "fillOpacity": 81,
                        "gradientMode": "opacity",
                        "hideFrom": {
                            "legend": false,
                            "tooltip": false,
                            "viz": false
                        },
                        "lineWidth": 3,
                        "stacking": {
                            "group": "A",
                            "mode": "none"
                        }
                    },
                    "mappings": [],
                    "min": 0,
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "red",
                                "value": 10
                            }
                        ]
                    },
                    "unit": "s"
                },
                "overrides": []
            },
            "gridPos": {
                "h": 6,
                "w": 16,
                "x": 8,
                "y": 8
            },
            "id": 4,
            "options": {
                "legend": {
                    "calcs": [],
                    "displayMode": "list",
                    "placement": "bottom",
                    "showLegend": false
                },
                "tooltip": {
                    "hideZeros": false,
                    "mode": "single",
                    "sort": "none"
                }
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "tempo",
                        "uid": "${tempo_datasource}"
                    },
                    "filters": [
                        {
                            "id": "status",
                            "operator": "=",
                            "scope": "intrinsic",
                            "tag": "status",
                            "value": "ok",
                            "valueType": "keyword"
                        },
                        {
                            "id": "f755ab99",
                            "operator": "=",
                            "scope": "span",
                            "tag": "telemetry.sdk.name",
                            "value": [
                                "openlit"
                            ],
                            "valueType": "string"
                        }
                    ],
                    "limit": 20,
                    "metricsQueryType": "range",
                    "query": "{status=ok && span.telemetry.sdk.name=\"openlit\" && span.service.name=~\"$application\" && span.deployment.environment=~\"$environment\"}",
                    "queryType": "traceql",
                    "refId": "A",
                    "tableType": "traces"
                }
            ],
            "title": "Request Duration Distribution",
            "type": "histogram"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the request volume over time, showing how LLM usage patterns change throughout the day. This helps identify peak usage periods and track adoption trends.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "mode": "palette-classic"
                    },
                    "custom": {
                        "axisBorderShow": false,
                        "axisCenteredZero": false,
                        "axisColorMode": "text",
                        "axisLabel": "",
                        "axisPlacement": "auto",
                        "barAlignment": 0,
                        "drawStyle": "line",
                        "fillOpacity": 20,
                        "gradientMode": "none",
                        "hideFrom": {
                            "legend": false,
                            "tooltip": false,
                            "viz": false
                        },
                        "insertNulls": false,
                        "lineInterpolation": "smooth",
                        "lineWidth": 2,
                        "pointSize": 5,
                        "scaleDistribution": {
                            "type": "linear"
                        },
                        "showPoints": "auto",
                        "spanNulls": false,
                        "stacking": {
                            "group": "A",
                            "mode": "none"
                        },
                        "thresholdsStyle": {
                            "mode": "off"
                        }
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "red",
                                "value": 80
                            }
                        ]
                    },
                    "unit": "reqps"
                },
                "overrides": []
            },
            "gridPos": {
                "h": 8,
                "w": 8,
                "x": 0,
                "y": 14
            },
            "id": 17,
            "options": {
                "legend": {
                    "calcs": [],
                    "displayMode": "list",
                    "placement": "bottom",
                    "showLegend": true
                },
                "tooltip": {
                    "mode": "single",
                    "sort": "none"
                }
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "rate(gen_ai_requests_total{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"}[5m])",
                    "hide": false,
                    "legendFormat": "Request Rate",
                    "range": true,
                    "refId": "A"
                }
            ],
            "title": "Request Volume Over Time",
            "transparent": true,
            "type": "timeseries"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the time to first token (TTFT) - a critical latency metric showing how quickly the LLM starts responding. Lower values indicate better perceived performance and user experience.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "mode": "palette-classic"
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "yellow",
                                "value": 0.5
                            },
                            {
                                "color": "red",
                                "value": 2
                            }
                        ]
                    },
                    "unit": "s",
                    "decimals": 3
                },
                "overrides": []
            },
            "gridPos": {
                "h": 4,
                "w": 8,
                "x": 8,
                "y": 14
            },
            "id": 16,
            "options": {
                "colorMode": "background",
                "graphMode": "area",
                "justifyMode": "auto",
                "orientation": "auto",
                "reduceOptions": {
                    "calcs": [
                        "lastNotNull"
                    ],
                    "fields": "",
                    "values": false
                },
                "textMode": "auto"
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "gen_ai_server_time_to_first_token_seconds_sum{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"} / gen_ai_server_time_to_first_token_seconds_count{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"}",
                    "hide": false,
                    "legendFormat": "Avg Time to First Token",
                    "range": true,
                    "refId": "A"
                }
            ],
            "title": "Time to First Token",
            "type": "stat"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays the token generation throughput - how many tokens per second the LLM is producing. Higher values indicate better performance and efficiency in token generation.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "mode": "palette-classic"
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "red",
                                "value": 0
                            },
                            {
                                "color": "yellow",
                                "value": 10
                            },
                            {
                                "color": "green",
                                "value": 50
                            }
                        ]
                    },
                    "unit": "tokens/sec",
                    "decimals": 1
                },
                "overrides": []
            },
            "gridPos": {
                "h": 4,
                "w": 8,
                "x": 16,
                "y": 14
            },
            "id": 13,
            "options": {
                "colorMode": "background",
                "graphMode": "area",
                "justifyMode": "auto",
                "orientation": "auto",
                "reduceOptions": {
                    "calcs": [
                        "lastNotNull"
                    ],
                    "fields": "",
                    "values": false
                },
                "textMode": "auto"
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "gen_ai_usage_output_tokens_total{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"} / (gen_ai_client_operation_duration_seconds_sum{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"} / gen_ai_client_operation_duration_seconds_count{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"})",
                    "hide": false,
                    "legendFormat": "Tokens/Second",
                    "range": true,
                    "refId": "A"
                }
            ],
            "title": "Token Generation Rate",
            "type": "stat"
        },
        {
            "datasource": {
                "type": "prometheus",
                "uid": "${prometheus_datasource}"
            },
            "description": "This panel displays a comparative graph showing the average number of tokens consumed for input and output against the average usage cost. It provides a visual representation of the relationship between the volume of data processed (in tokens) and the financial implications of using GenAI services. Analyzing this comparison helps in assessing cost-effectiveness and guiding strategic decisions for efficient resource utilization.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "mode": "palette-classic"
                    },
                    "custom": {
                        "axisBorderShow": false,
                        "axisCenteredZero": false,
                        "axisColorMode": "text",
                        "axisLabel": "",
                        "axisPlacement": "auto",
                        "barAlignment": 1,
                        "barWidthFactor": 0.6,
                        "drawStyle": "line",
                        "fillOpacity": 30,
                        "gradientMode": "opacity",
                        "hideFrom": {
                            "legend": false,
                            "tooltip": false,
                            "viz": false
                        },
                        "insertNulls": false,
                        "lineInterpolation": "smooth",
                        "lineStyle": {
                            "fill": "solid"
                        },
                        "lineWidth": 2,
                        "pointSize": 5,
                        "scaleDistribution": {
                            "type": "linear"
                        },
                        "showPoints": "always",
                        "spanNulls": true,
                        "stacking": {
                            "group": "A",
                            "mode": "none"
                        },
                        "thresholdsStyle": {
                            "mode": "off"
                        }
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            },
                            {
                                "color": "red",
                                "value": 80
                            }
                        ]
                    },
                    "unit": "none"
                },
                "overrides": [
                    {
                        "matcher": {
                            "id": "byName",
                            "options": "Usage Cost"
                        },
                        "properties": [
                            {
                                "id": "unit",
                                "value": "currencyUSD"
                            },
                            {
                                "id": "decimals",
                                "value": 6
                            }
                        ]
                    }
                ]
            },
            "gridPos": {
                "h": 4,
                "w": 16,
                "x": 8,
                "y": 18
            },
            "id": 6,
            "options": {
                "legend": {
                    "calcs": [],
                    "displayMode": "list",
                    "placement": "bottom",
                    "showLegend": true
                },
                "tooltip": {
                    "hideZeros": false,
                    "mode": "single",
                    "sort": "none"
                }
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "avg(gen_ai_usage_input_tokens_total{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"})",
                    "hide": false,
                    "legendFormat": "Input Tokens",
                    "range": true,
                    "refId": "A"
                },
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "avg(gen_ai_usage_output_tokens_total{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"})",
                    "hide": false,
                    "legendFormat": "Output Tokens",
                    "range": true,
                    "refId": "B"
                },
                {
                    "datasource": {
                        "type": "prometheus",
                        "uid": "${prometheus_datasource}"
                    },
                    "expr": "avg(gen_ai_usage_cost_USD_sum{telemetry_sdk_name=\"openlit\", service_name=~\"$application\", deployment_environment=~\"$environment\"}) / 100000 OR on() vector(0)",
                    "hide": false,
                    "legendFormat": "Usage Cost",
                    "range": true,
                    "refId": "C"
                }
            ],
            "title": "Average Token Consumption vs. Average Usage Cost Comparison",
            "type": "timeseries"
        },
        {
            "collapsed": false,
            "gridPos": {
                "h": 1,
                "w": 24,
                "x": 0,
                "y": 22
            },
            "id": 10,
            "panels": [],
            "title": "GenAI Requests",
            "type": "row"
        },
        {
            "datasource": {
                "type": "tempo",
                "uid": "${tempo_datasource}"
            },
            "description": "This panel displays a detailed table of GenAI request traces, providing comprehensive insights into each request's timing, source, type, and outcome. It allows for the in-depth analysis of individual requests, facilitating troubleshooting, performance monitoring, and understanding user interactions with GenAI services. Tracking this helps in identifying patterns, potential issues, and opportunities for optimization.",
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "mode": "continuous-BlPu"
                    },
                    "custom": {
                        "align": "auto",
                        "cellOptions": {
                            "type": "color-background"
                        },
                        "inspect": false
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": 0
                            }
                        ]
                    },
                    "unit": "s"
                },
                "overrides": []
            },
            "gridPos": {
                "h": 12,
                "w": 24,
                "x": 0,
                "y": 23
            },
            "id": 8,
            "options": {
                "cellHeight": "sm",
                "footer": {
                    "countRows": false,
                    "fields": "",
                    "reducer": [
                        "sum"
                    ],
                    "show": false
                },
                "showHeader": true
            },
            "pluginVersion": "12.1.0-91038.patch3-91166",
            "targets": [
                {
                    "datasource": {
                        "type": "tempo",
                        "uid": "${tempo_datasource}"
                    },
                    "filters": [
                        {
                            "id": "status",
                            "operator": "=",
                            "scope": "intrinsic",
                            "tag": "status",
                            "value": "ok",
                            "valueType": "keyword"
                        }
                    ],
                    "limit": 20,
                    "metricsQueryType": "range",
                    "query": "{status=ok && span.service.name=~\"$application\" && span.deployment.environment=~\"$environment\"} | select(span.gen_ai.prompt, span.gen_ai.completion, span.gen_ai.usage.cost)",
                    "queryType": "traceql",
                    "refId": "A",
                    "tableType": "traces"
                }
            ],
            "title": "",
            "transparent": true,
            "type": "table"
        }
    ],
    "preload": false,
    "refresh": "30s",
    "schemaVersion": 41,
    "tags": [
        "ai-observability-integration"
    ],
    "templating": {
        "list": [
            {
                "current": {
                    "text": "grafanacloud-gjvengelen-traces",
                    "value": "grafanacloud-traces"
                },
                "includeAll": false,
                "label": "Tempo data source",
                "name": "tempo_datasource",
                "options": [],
                "query": "tempo",
                "refresh": 1,
                "regex": "",
                "type": "datasource"
            },
            {
                "current": {
                    "text": "grafanacloud-gjvengelen-prom",
                    "value": "grafanacloud-prom"
                },
                "includeAll": false,
                "label": "Prometheus data source",
                "name": "prometheus_datasource",
                "options": [],
                "query": "prometheus",
                "refresh": 1,
                "regex": "(?!grafanacloud-usage|grafanacloud-ml-metrics).+",
                "type": "datasource"
            },
            {
                "allValue": ".*",
                "current": {
                    "text": "All",
                    "value": "$__all"
                },
                "datasource": {
                    "type": "prometheus",
                    "uid": "${prometheus_datasource}"
                },
                "definition": "label_values(gen_ai_requests_total, service_name)",
                "includeAll": true,
                "label": "Application",
                "multi": true,
                "name": "application",
                "options": [],
                "query": {
                    "qryType": 1,
                    "query": "label_values(gen_ai_requests_total, service_name)",
                    "refId": "PrometheusVariableQueryEditor-VariableQuery"
                },
                "refresh": 2,
                "regex": "",
                "type": "query"
            },
            {
                "allValue": ".*",
                "current": {
                    "text": "All",
                    "value": "$__all"
                },
                "datasource": {
                    "type": "prometheus",
                    "uid": "${prometheus_datasource}"
                },
                "definition": "label_values(gen_ai_requests_total, deployment_environment)",
                "includeAll": true,
                "label": "Environment",
                "multi": true,
                "name": "environment",
                "options": [],
                "query": {
                    "qryType": 1,
                    "query": "label_values(gen_ai_requests_total, deployment_environment)",
                    "refId": "PrometheusVariableQueryEditor-VariableQuery"
                },
                "refresh": 2,
                "regex": "",
                "sort": 1,
                "type": "query"
            }
        ]
    },
    "time": {
        "from": "now-30m",
        "to": "now"
    },
    "timepicker": {},
    "timezone": "browser",
    "title": "GenAI Observability",
    "uid": "cdiz9piuoa3gge",
    "version": 16
}